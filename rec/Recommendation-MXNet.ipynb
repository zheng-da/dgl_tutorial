{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems with DGL\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Graph Neural Networks (GNN), as a methodology of learning representations on graphs, has gained much attention recently.  Various models such as Graph Convolutional Networks, GraphSAGE, etc. are proposed to obtain representations of whole graphs, or nodes on a single graph.\n",
    "\n",
    "A primary goal of Collaborative Filtering (CF) is to automatically make predictions about a user's interest, e.g. whether/how a user would interact with a set of items, given the interaction history of the user herself, as well as the histories of other users.  The user-item interaction can also be viewed as a bipartite graph, where users and items form two sets of nodes, and edges connecting them stands for interactions.  The problem can then be formulated as a *link-prediction* problem, where we try to predict whether an edge (of a given type) exists between two nodes.\n",
    "\n",
    "Based on this intuition, the academia developed multiple new models for CF, including but not limited to:\n",
    "\n",
    "* Geometric Learning Approaches\n",
    "  * [Geometric Matrix Completion](https://papers.nips.cc/paper/5938-collaborative-filtering-with-graph-information-consistency-and-scalable-methods.pdf)\n",
    "  * [Recurrent Multi-graph CNN](https://arxiv.org/pdf/1704.06803.pdf)\n",
    "* Graph-convolutional Approaches\n",
    "  * Models such as [R-GCN](https://arxiv.org/pdf/1703.06103.pdf) or [GraphSAGE](https://github.com/stellargraph/stellargraph/tree/develop/demos/link-prediction/hinsage) also apply.\n",
    "  * [Graph Convolutional Matrix Completion](https://arxiv.org/abs/1706.02263)\n",
    "  * [PinSage](https://arxiv.org/pdf/1806.01973.pdf)\n",
    "  \n",
    "In this hands-on tutorial, we will demonstrate how to write GraphSAGE in DGL + MXNet, and how to apply it in a recommender system setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "* Latest DGL release: `conda install -c dglteam dgl`\n",
    "* `pandas`\n",
    "* `stanfordnlp`\n",
    "* `mxnet`\n",
    "* `tqdm` for displaying the progress bar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "In this tutorial, we focus on rating prediction on MovieLens-1M dataset.  The data comes from [MovieLens](http://files.grouplens.org/datasets/movielens/ml-1m.zip) and is shipped with the notebook already.\n",
    "\n",
    "After loading and train-validation-test-splitting the dataset, we process the movie title into (padded) word-ID sequences, and other features into categorical variables (i.e. integers).  We then store them as node features on the graph.\n",
    "\n",
    "Since user features and item features are different, we pad both types of features with zeros.\n",
    "\n",
    "All of the above is encapsulated in `movielens.MovieLens` class for clarity of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DGLBACKEND'] = 'mxnet'\n",
    "import mxnet as mx\n",
    "from mxnet import ndarray as nd, autograd, gluon\n",
    "from mxnet.gluon import nn\n",
    "import dgl\n",
    "import dgl.function as FN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1682 [00:00<00:15, 107.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/gq/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/gq/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "Done loading processors!\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1682/1682 [00:16<00:00, 99.00it/s] \n",
      "100%|██████████| 1682/1682 [00:00<00:00, 2306.27it/s]\n",
      "/home/gq/miniconda3/lib/python3.7/site-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 943/943 [00:01<00:00, 713.64it/s]\n"
     ]
    }
   ],
   "source": [
    "import movielens_mx\n",
    "import stanfordnlp\n",
    "\n",
    "# IMPORTANT!!!\n",
    "# If you don't have stanfordnlp installed and the English models downloaded, please uncomment this statement\n",
    "#stanfordnlp.download('en', force=True)\n",
    "\n",
    "ml = movielens_mx.MovieLens('ml-100k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We can now write a GraphSAGE layer.  In GraphSAGE, the node representation is updated with the representation in the previous layer as well as an aggregation (often mean) of \"messages\" sent from all neighboring nodes.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "The algorithm of a single GraphSAGE layer goes as follows for each node $v$:\n",
    "\n",
    "1. $h_{\\mathcal{N}(v)} \\gets \\mathtt{Average}_{u \\in \\mathcal{N}(v)} h_{u}$\n",
    "2. $h_{v} \\gets \\sigma\\left(W \\cdot \\mathtt{CONCAT}(h_v, h_{\\mathcal{N}(v)})\\right)$\n",
    "3. $h_{v} \\gets h_{v} / \\lVert h_{v} \\rVert_2$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\mathtt{Average}$ can be replaced by any kind of aggregation including `sum`, `max`, or even an LSTM.\n",
    "* $\\sigma$ is any non-linearity function (e.g. `LeakyReLU`)\n",
    "\n",
    "We simply repeat the computation above for multiple GraphSAGE layers.\n",
    "\n",
    "### DGL Message Passing\n",
    "\n",
    "DGL adopts the message-passing paradigm, or scatter-apply-gather paradigm, for feature computation on a graph.  It decomposes the computation into three stages:\n",
    "\n",
    "1. *Message computation*: each edge is computed a message according to features on the edge itself, as well as the features on its source and destination node.  Often times, the message computation simply involves copying the representation of the source node.\n",
    "2. *Message aggregation*: each node then \"receives\" the messages sent from its neighbors, and call a function which reduces these messages into a single representation independent of the number of neighbors.  Averaging and summing are two of the most common message aggregation functions.\n",
    "3. *Node feature update*: with an aggregated representation from the neighbors, a node then updates its own representation using the aggregation.\n",
    "\n",
    "With the three stages in mind, we can easily figure out how to map the GraphSAGE layer computation into the message-passing paradigm:\n",
    "\n",
    "1. $h_{\\mathcal{N}(v)} \\gets \\underbrace{\\mathtt{Average}_{u \\in \\mathcal{N}(v)} \\underbrace{h_{u}}_{\\text{Message computation (copy from source)}}}_{\\text{Message aggregation}}$\n",
    "2. $h_{v} \\gets \\underbrace{\\sigma\\left(W \\cdot \\mathtt{CONCAT}(h_v, h_{\\mathcal{N}(v)})\\right)}_{\\text{Node feature update}}$\n",
    "3. $h_{v} \\gets \\underbrace{h_{v} / \\lVert h_{v} \\rVert_2}_{\\text{Node feature update}}$\n",
    "\n",
    "While DGL does not provide the $\\mathtt{Average}$ aggregation function yet (as it's a future work item), it does provide the $\\mathtt{Sum}$ aggregation.  So we can modify the algorithm above to the following that is readily to be implemented in DGL:\n",
    "\n",
    "1. $d_{\\mathcal{N}(v)} \\gets \\underbrace{\\mathtt{Sum}_{u \\in \\mathcal{N}(v)} \\underbrace{1}_{\\text{Message computation (copy from source)}}}_{\\text{Message aggregation}}$\n",
    "2. $h_{\\mathcal{N}(v)} \\gets \\underbrace{\\mathtt{Sum}_{u \\in \\mathcal{N}(v)} \\underbrace{h_{u}}_{\\text{Message computation (copy from source)}}}_{\\text{Message aggregation}}$\n",
    "3. $h_{v} \\gets \\underbrace{\\sigma\\left(W \\cdot \\mathtt{CONCAT}(h_v, h_{\\mathcal{N}(v)} / d_{\\mathcal{N}(v)})\\right)}_{\\text{Node feature update}}$\n",
    "4. $h_{v} \\gets \\underbrace{h_{v} / \\lVert h_{v} \\rVert_2}_{\\text{Node feature update}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Ideally, we wish to execute a full update of the node embeddings with the GraphSAGE layer.  However, when the graph scales up, the full update soon becomes impractical, because the node embeddings couldn't fit in the GPU memory.\n",
    "\n",
    "A natural solution would be partitioning the nodes and computing the embeddings one partition (minibatch) at a time.  The nodes at one convolution layer then only depends on their neighbors, rather than all the nodes in the graph, hence reducing the computational cost.  However, if we have multiple layers, and some of the nodes have a lot of neighbors (which is often the case since the degree distribution of many real-world graphs follow [power-law](https://en.wikipedia.org/wiki/Scale-free_network)), then the computation may still eventually depend on every node in the graph.\n",
    "\n",
    "*Neighbor sampling* is an answer to further reduce the cost of computing node embeddings.  When aggregating messages, instead of collecting from all neighboring nodes, we only collect from some of the randomly-sampled (for instance, uniform sampling at most K neighbors without replacement) neighbors.\n",
    "\n",
    "DGL provides the `NodeFlow` object that describes the computation dependency of nodes in a graph convolutional network, as well as various samplers that constructs such `NodeFlow`s as graphs.  From a programmer's perspective, training with minibatch and neighbor sampling reduces to propagating the messages in `NodeFlow` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_embeddings(ndata, gcn):\n",
    "    \"\"\"Adds external (categorical and numeric) features into node representation G.ndata['h']\"\"\"\n",
    "    extra_repr = []\n",
    "    for key, value in ndata.items():\n",
    "        if (value.dtype == np.int64) and key in gcn.emb:\n",
    "            result = getattr(gcn, 'emb_' + key)(value)\n",
    "            if result.ndim == 3:    # bag of words: the result would be a (n_nodes x seq_len x feature_size) tensor\n",
    "                mask = value != 0\n",
    "                result = (result * mask.expand_dims(2).astype(float)).sum(1) / mask.sum(1)\n",
    "            extra_repr.append(result)\n",
    "        elif (value.dtype == np.float32) and key in gcn.proj:\n",
    "            result = getattr(gcn, 'proj_' + key)(value)\n",
    "            extra_repr.append(result)\n",
    "    ndata['h'] = ndata['h'] + nd.stack(*extra_repr, axis=0).sum(axis=0)\n",
    "\n",
    "class GraphSageConvWithSampling(nn.Block):\n",
    "    def __init__(self, feature_size):\n",
    "        super(GraphSageConvWithSampling, self).__init__()\n",
    "\n",
    "        self.feature_size = feature_size\n",
    "        self.W = nn.Dense(feature_size)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        h_agg = nodes.data['h_agg']\n",
    "        h = nodes.data['h']\n",
    "        w = nodes.data['w'].expand_dims(1)\n",
    "        # HACK 1:\n",
    "        # When computing the representation of node v on layer L, we would like to\n",
    "        # include the dependency of node v itself on layer L-1.  However, we don't\n",
    "        # want to aggregate node v's own \"message\".  So we tell the sampler to\n",
    "        # always \"add self loop\" to include such dependency, but we subtract the\n",
    "        # node's representation from aggregation later.\n",
    "        h_agg = h_agg - h / nd.maximum((w - 1), 1e-6)    # HACK 1\n",
    "        h_concat = nd.concat(h, h_agg, dim=1)\n",
    "        h_new = self.leaky_relu(self.W(h_concat))\n",
    "        return {'h': h_new / nd.maximum(h_new.norm(axis=1, keepdims=True), 1e-6)}\n",
    "    \n",
    "class GraphSageWithSampling(nn.Block):\n",
    "    def __init__(self, feature_size, n_layers, G):\n",
    "        super(GraphSageWithSampling, self).__init__()\n",
    "        \n",
    "        self.feature_size = feature_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Simulating ModuleList\n",
    "        for i in range(n_layers):\n",
    "            setattr(self, 'conv_%d' % i, GraphSageConvWithSampling(feature_size))\n",
    "        self.emb = set()\n",
    "        self.proj = set()\n",
    "\n",
    "        for key, scheme in G.node_attr_schemes().items():\n",
    "            if scheme.dtype == np.int64:\n",
    "                n_items = G.ndata[key].max().asscalar()\n",
    "                # Simulating ModuleDict\n",
    "                self.emb.add(key)\n",
    "                setattr(self,\n",
    "                        'emb_' + key,\n",
    "                        nn.Embedding(\n",
    "                            n_items + 1,\n",
    "                            self.feature_size))\n",
    "            elif scheme.dtype == np.float32:\n",
    "                # Simulating ModuleDict\n",
    "                self.proj.add(key)\n",
    "                seq = nn.Sequential()\n",
    "                with seq.name_scope():\n",
    "                    w = nn.Dense(self.feature_size)\n",
    "                    seq.add(w)\n",
    "                    seq.add(nn.LeakyReLU(0.1))\n",
    "                setattr(self, 'proj_' + key, seq)\n",
    "                \n",
    "        self.G = G\n",
    "        \n",
    "        self.node_emb = nn.Embedding(G.number_of_nodes() + 1, feature_size)\n",
    "\n",
    "    msg = [FN.copy_src('h', 'h'),\n",
    "           FN.copy_src('one', 'one')]\n",
    "    red = [FN.sum('h', 'h_agg'), FN.sum('one', 'w')]\n",
    "\n",
    "    def forward(self, nf):\n",
    "        '''\n",
    "        nf: NodeFlow.\n",
    "        '''\n",
    "        nf.copy_from_parent(edge_embed_names=None)\n",
    "        for i in range(nf.num_layers):\n",
    "            nf.layers[i].data['h'] = self.node_emb(nf.layer_parent_nid(i) + 1)\n",
    "            nf.layers[i].data['one'] = nd.ones(nf.layer_size(i))\n",
    "            mix_embeddings(nf.layers[i].data, self)\n",
    "        if self.n_layers == 0:\n",
    "            return nf.layers[i].data['h']\n",
    "        for i in range(self.n_layers):\n",
    "            nf.block_compute(i, self.msg, self.red, getattr(self, 'conv_%d' % i))\n",
    "\n",
    "        result = nf.layers[self.n_layers].data['h']\n",
    "        assert (result != result).sum() == 0\n",
    "        return result\n",
    "    \n",
    "class GraphSAGERecommender(nn.Block):\n",
    "    def __init__(self, gcn):\n",
    "        super(GraphSAGERecommender, self).__init__()\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.gcn = gcn\n",
    "            self.node_biases = self.params.get(\n",
    "                'node_biases',\n",
    "                init=mx.init.Zero(),\n",
    "                shape=(gcn.G.number_of_nodes()+1,))\n",
    "        \n",
    "    def forward(self, nf, src, dst):\n",
    "        h_output = self.gcn(nf)\n",
    "        h_src = h_output[nodeflow.map_from_parent_nid(-1, src, True)]\n",
    "        h_dst = h_output[nodeflow.map_from_parent_nid(-1, dst, True)]\n",
    "        score = (h_src * h_dst).sum(1) + self.node_biases.data()[src+1] + self.node_biases.data()[dst+1]\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "As above, training now only involves\n",
    "1. Initializing a sampler\n",
    "2. Iterating over the neighbor sampler, propagating the messages, and computing losses and gradients as usual.\n",
    "\n",
    "Meanwhile, we also evaluate the RMSE on validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gq/miniconda3/lib/python3.7/site-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n",
      "/home/gq/miniconda3/lib/python3.7/site-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 6.347813 Validation RMSE: 2.585861 Test RMSE: 2.558773\n",
      "Training loss: 5.542541 Validation RMSE: 2.4062366 Test RMSE: 2.3787484\n",
      "Training loss: 5.4501443 Validation RMSE: 2.2445116 Test RMSE: 2.2168806\n",
      "Training loss: 4.5191236 Validation RMSE: 2.0974317 Test RMSE: 2.0699387\n",
      "Training loss: 3.7660592 Validation RMSE: 1.9635228 Test RMSE: 1.9363042\n",
      "Training loss: 3.0640545 Validation RMSE: 1.8413929 Test RMSE: 1.8146951\n",
      "Training loss: 3.1744337 Validation RMSE: 1.7300578 Test RMSE: 1.7041215\n",
      "Training loss: 2.7352517 Validation RMSE: 1.6289572 Test RMSE: 1.604018\n",
      "Training loss: 2.3519938 Validation RMSE: 1.537973 Test RMSE: 1.5143223\n",
      "Training loss: 2.2558618 Validation RMSE: 1.4564359 Test RMSE: 1.4343337\n",
      "Training loss: 2.0855591 Validation RMSE: 1.3838452 Test RMSE: 1.3633515\n",
      "Training loss: 1.6585584 Validation RMSE: 1.3196548 Test RMSE: 1.3009329\n",
      "Training loss: 1.8998642 Validation RMSE: 1.2637489 Test RMSE: 1.2467239\n",
      "Training loss: 1.2321182 Validation RMSE: 1.2146727 Test RMSE: 1.1995001\n",
      "Training loss: 1.1816047 Validation RMSE: 1.1726366 Test RMSE: 1.1593385\n",
      "Training loss: 1.0963283 Validation RMSE: 1.1364161 Test RMSE: 1.1250213\n",
      "Training loss: 1.1466604 Validation RMSE: 1.1053007 Test RMSE: 1.0960528\n",
      "Training loss: 1.0738839 Validation RMSE: 1.0790263 Test RMSE: 1.0717654\n",
      "Training loss: 1.3001776 Validation RMSE: 1.0568283 Test RMSE: 1.0513687\n",
      "Training loss: 1.0067028 Validation RMSE: 1.0384872 Test RMSE: 1.0346644\n",
      "Training loss: 1.1757929 Validation RMSE: 1.0229203 Test RMSE: 1.020542\n",
      "Training loss: 0.921376 Validation RMSE: 1.0103296 Test RMSE: 1.0092363\n",
      "Training loss: 0.95435786 Validation RMSE: 0.99968696 Test RMSE: 0.99986136\n",
      "Training loss: 1.0062828 Validation RMSE: 0.99083513 Test RMSE: 0.99195516\n",
      "Training loss: 1.1394486 Validation RMSE: 0.98335373 Test RMSE: 0.9855037\n",
      "Training loss: 0.9441379 Validation RMSE: 0.97704655 Test RMSE: 0.9798573\n",
      "Training loss: 0.6752957 Validation RMSE: 0.9716715 Test RMSE: 0.9752548\n",
      "Training loss: 0.79278797 Validation RMSE: 0.967236 Test RMSE: 0.97130924\n",
      "Training loss: 0.8187455 Validation RMSE: 0.96352017 Test RMSE: 0.9681364\n",
      "Training loss: 0.98305327 Validation RMSE: 0.9602534 Test RMSE: 0.9652468\n",
      "Training loss: 0.83799815 Validation RMSE: 0.9573567 Test RMSE: 0.9628159\n",
      "Training loss: 0.95147437 Validation RMSE: 0.95500827 Test RMSE: 0.96070695\n",
      "Training loss: 1.1341008 Validation RMSE: 0.95287377 Test RMSE: 0.9589619\n",
      "Training loss: 1.0484791 Validation RMSE: 0.9511173 Test RMSE: 0.9572835\n",
      "Training loss: 1.072005 Validation RMSE: 0.94955 Test RMSE: 0.95593756\n",
      "Training loss: 0.92275 Validation RMSE: 0.9483376 Test RMSE: 0.9549393\n",
      "Training loss: 0.7619531 Validation RMSE: 0.94708985 Test RMSE: 0.95383704\n",
      "Training loss: 0.97070116 Validation RMSE: 0.9460849 Test RMSE: 0.9527946\n",
      "Training loss: 0.70610994 Validation RMSE: 0.94466335 Test RMSE: 0.95124894\n",
      "Training loss: 0.8263134 Validation RMSE: 0.94101495 Test RMSE: 0.9447285\n",
      "Training loss: 0.77483356 Validation RMSE: 0.9343007 Test RMSE: 0.9386319\n",
      "Training loss: 0.9408968 Validation RMSE: 0.9299216 Test RMSE: 0.9345646\n",
      "Training loss: 0.75363636 Validation RMSE: 0.9231672 Test RMSE: 0.92829263\n",
      "Training loss: 0.7364888 Validation RMSE: 0.92142624 Test RMSE: 0.92832714\n",
      "Training loss: 0.6967131 Validation RMSE: 0.92018557 Test RMSE: 0.9260456\n",
      "Training loss: 0.79943943 Validation RMSE: 0.9184906 Test RMSE: 0.92557985\n",
      "Training loss: 0.74417186 Validation RMSE: 0.9177432 Test RMSE: 0.9249152\n",
      "Training loss: 0.8833292 Validation RMSE: 0.91664654 Test RMSE: 0.9248627\n",
      "Training loss: 0.79793113 Validation RMSE: 0.9169529 Test RMSE: 0.92517406\n",
      "Training loss: 0.84344333 Validation RMSE: 0.9151609 Test RMSE: 0.9234251\n"
     ]
    }
   ],
   "source": [
    "g = ml.g\n",
    "# Find the subgraph of all \"training\" edges\n",
    "g_train = g.edge_subgraph(g.filter_edges(lambda edges: edges.data['train']).astype('int64'), True)\n",
    "g_train.copy_from_parent()\n",
    "g_train.readonly()\n",
    "eid_valid = g.filter_edges(lambda edges: edges.data['valid']).astype('int64')\n",
    "eid_test = g.filter_edges(lambda edges: edges.data['test']).astype('int64')\n",
    "src_valid, dst_valid = g.find_edges(eid_valid)\n",
    "src_test, dst_test = g.find_edges(eid_test)\n",
    "src, dst = g_train.all_edges()\n",
    "rating = g_train.edata['rating']\n",
    "rating_valid = g.edges[eid_valid].data['rating']\n",
    "rating_test = g.edges[eid_test].data['rating']\n",
    "\n",
    "model = GraphSAGERecommender(GraphSageWithSampling(100, 1, g_train))\n",
    "model.collect_params().initialize(ctx=mx.cpu())\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': 0.001, 'wd': 1e-9})\n",
    "\n",
    "batch_size = 1024\n",
    "n_users = len(ml.user_ids)\n",
    "n_products = len(ml.product_ids)\n",
    "\n",
    "for epoch in range(50):\n",
    "    shuffle_idx = nd.from_numpy(np.random.permutation(g_train.number_of_edges()))\n",
    "    src_shuffled = src[shuffle_idx]\n",
    "    dst_shuffled = dst[shuffle_idx]\n",
    "    rating_shuffled = rating[shuffle_idx]\n",
    "    src_batches = []\n",
    "    dst_batches = []\n",
    "    rating_batches = []\n",
    "    for i in range(0, g_train.number_of_edges(), batch_size):\n",
    "        j = min(i + batch_size, g_train.number_of_edges())\n",
    "        src_batches.append(src_shuffled[shuffle_idx[i:j]])\n",
    "        dst_batches.append(dst_shuffled[shuffle_idx[i:j]])\n",
    "        rating_batches.append(rating_shuffled[shuffle_idx[i:j]])\n",
    "    \n",
    "    # HACK 2: Alternate between source batch and destination batch, so we can put exactly\n",
    "    # a batch of edges' endpoints in a single NodeFlow.\n",
    "    seed_nodes = nd.concat(*sum([[s, d] for s, d in zip(src_batches, dst_batches)], []), dim=0)\n",
    "    \n",
    "    sampler = dgl.contrib.sampling.NeighborSampler(\n",
    "        g_train,               # the graph\n",
    "        batch_size * 2,        # number of nodes to compute at a time, HACK 2\n",
    "        5,                     # number of neighbors for each node\n",
    "        1,                     # number of layers in GCN\n",
    "        seed_nodes=seed_nodes, # list of seed nodes, HACK 2\n",
    "        prefetch=True,         # whether to prefetch the NodeFlows\n",
    "        add_self_loop=True,    # whether to add a self-loop in the NodeFlows, HACK 1\n",
    "        shuffle=False,         # whether to shuffle the seed nodes.  Should be False here.\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    for s, d, r, nodeflow in zip(src_batches, dst_batches, rating_batches, sampler):\n",
    "        with mx.autograd.record():\n",
    "            score = model.forward(nodeflow, s, d)\n",
    "            loss = ((score - r) ** 2).mean()\n",
    "            loss.backward()\n",
    "        trainer.step(s.shape[0])\n",
    "\n",
    "    # Validation & Test, we precompute GraphSage output for all nodes first.\n",
    "    sampler = dgl.contrib.sampling.NeighborSampler(\n",
    "        g_train,\n",
    "        batch_size,\n",
    "        5,\n",
    "        1,\n",
    "        seed_nodes=nd.arange(g.number_of_nodes()).astype('int64'),\n",
    "        prefetch=True,\n",
    "        add_self_loop=True,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    h = []\n",
    "    for nf in sampler:\n",
    "        h.append(model.gcn(nf))\n",
    "    h = nd.concat(*h, dim=0)\n",
    "\n",
    "    # Compute validation RMSE\n",
    "    score = nd.zeros(len(src_valid))\n",
    "    for i in range(0, len(src_valid), batch_size):\n",
    "        j = min(i + batch_size, len(src_valid))\n",
    "        s = src_valid[i:j]\n",
    "        d = dst_valid[i:j]\n",
    "        node_biases = model.node_biases.data()\n",
    "        score[i:j] = (h[s] * h[d]).sum(1) + node_biases[s + 1] + node_biases[d + 1]\n",
    "    valid_rmse = nd.sqrt(((score - rating_valid) ** 2).mean())\n",
    "\n",
    "    # Compute test RMSE\n",
    "    score = nd.zeros(len(src_test))\n",
    "    for i in range(0, len(src_test), batch_size):\n",
    "        j = min(i + batch_size, len(src_test))\n",
    "        s = src_test[i:j]\n",
    "        d = dst_test[i:j]\n",
    "        node_biases = model.node_biases.data()\n",
    "        score[i:j] = (h[s] * h[d]).sum(1) + node_biases[s + 1] + node_biases[d + 1]\n",
    "    test_rmse = nd.sqrt(((score - rating_test) ** 2).mean())\n",
    "\n",
    "    print('Training loss:', loss.asscalar(), 'Validation RMSE:', valid_rmse.asscalar(), 'Test RMSE:', test_rmse.asscalar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
