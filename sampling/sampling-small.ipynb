{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl.data import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Load a small dataset\n",
    "\n",
    "cora is a small graph of 2708 nodes and 10556 edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataArgs = namedtuple('DataArgs', 'dataset')\n",
    "args = DataArgs('cora')\n",
    "data = load_data(args)\n",
    "\n",
    "print(\"\"\"----Data statistics------'\n",
    "      #Nodes %d\n",
    "      #Edges %d\n",
    "      #Classes %d\n",
    "      #Train samples %d\n",
    "      #Val samples %d\n",
    "      #Test samples %d\"\"\" %\n",
    "          (data.graph.number_of_nodes(),\n",
    "           data.graph.number_of_edges(),\n",
    "           data.num_labels,\n",
    "           data.train_mask.sum(),\n",
    "           data.val_mask.sum(),\n",
    "           data.test_mask.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Load data into DGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl.DGLGraph(data.graph, readonly=True)\n",
    "g.ndata['features'] = mx.nd.array(data.features)\n",
    "g.ndata['labels'] = mx.nd.array(data.labels)\n",
    "\n",
    "train_mask = mx.nd.array(data.train_mask)\n",
    "val_mask = mx.nd.array(data.val_mask)\n",
    "test_mask = mx.nd.array(data.test_mask)\n",
    "\n",
    "train_nid = mx.nd.array(np.nonzero(data.train_mask)[0]).astype(np.int64)\n",
    "test_nid = mx.nd.array(np.nonzero(data.test_mask)[0]).astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Compute a node embedding with  2-layer GCN in mini-batches\n",
    "\n",
    "To enable mini-batch training, we need to compute the node embeddings on a subset of nodes. For a 2-layer GCN, we first compute the node embeddings of the neighbors in the first layer of the GCN model.\n",
    "\n",
    "$$h_i^{(1)} = \\sigma(\\Sigma_{j \\in N(i)} h_j^{(0)}), \\forall i \\in N(v)$$\n",
    "\n",
    "Then we compute the node embeddings of the second layer, which is the final embeddings of the nodes in this model.\n",
    "\n",
    "$$h_v^{(2)} = \\sigma(\\Sigma_{i \\in N(v)} h_i^{(1)})$$\n",
    "\n",
    "The data and computation dependency is illustrated in the figure below:\n",
    "![title](https://s3.us-east-2.amazonaws.com/dgl.ai/amlc_tutorial/Dependency.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We could use the DGL API from the previous session to implement the mini-batch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeUpdate(gluon.Block):\n",
    "    def __init__(self, in_feats, out_feats, activation=None):\n",
    "        super(NodeUpdate, self).__init__()\n",
    "        self.dense = gluon.nn.Dense(out_feats, in_units=in_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = node.data['h']\n",
    "        h = self.dense(h)\n",
    "        if self.activation:\n",
    "            h = self.activation(h)\n",
    "        return {'h': h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Input dimension size\n",
    "in_feats = data.features.shape[1]\n",
    "# Hidden dimension size\n",
    "n_hidden = 64\n",
    "\n",
    "# If we want to compute the embeddings of the nodes 0, 1, 2\n",
    "v = [0, 1, 2]\n",
    "\n",
    "nfunc_layer1 = NodeUpdate(in_feats, n_hidden, mx.nd.relu)\n",
    "nfunc_layer2 = NodeUpdate(n_hidden, n_hidden, mx.nd.relu)\n",
    "nfunc_layer1.initialize()\n",
    "nfunc_layer2.initialize()\n",
    "\n",
    "# We first find the neighbors of these nodes.\n",
    "neighbors = np.unique(g.in_edges(v)[0].asnumpy())\n",
    "# compute the embeddings of the neighbors\n",
    "g.pull(neighbors, fn.copy_src('features', 'msg'), fn.sum('msg', 'h'), nfunc_layer1)\n",
    "# compute the embeddings of the nodes in the batch.\n",
    "g.pull(v, fn.copy_src('h', 'msg'), fn.sum('msg', 'h'), nfunc_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The computation above is very expensive because computing the node embeddings of the model involve in many neighbor nodes. One solution is to use neighbor sampling to prune some of the dependency.\n",
    "\n",
    "![title](https://s3.us-east-2.amazonaws.com/dgl.ai/amlc_tutorial/neighbor_sampling.png)\n",
    "\n",
    "Neighbor sampling requires to perform pruning in every neighborhood separately. We could implement neighbor sampling with Python as follow. However, the code below is tedious to implement and is also very slow when running on a large graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# If we want to compute the embeddings of the nodes 0, 1, 2\n",
    "nodes = [0, 1, 2]\n",
    "num_neighs = 2\n",
    "\n",
    "# Sample neighbors in the neighborhood of each node separately.\n",
    "for n in nodes:\n",
    "    src2, dst2 = g.in_edges(n)\n",
    "    # Randomly select neighbors\n",
    "    idx = np.random.choice(np.arange(len(src2)), size=num_neighs)\n",
    "    src2 = src2[idx]\n",
    "    dst2 = dst2[idx]\n",
    "    \n",
    "    # Sample neighbors of neighbors.\n",
    "    for neighbor in src2:\n",
    "        src1, dst1 = g.in_edges(neighbor)\n",
    "        idx = np.random.choice(np.arange(len(src1)), size=num_neighs)\n",
    "        src1 = src1[idx]\n",
    "        dst1 = dst1[idx]\n",
    "        g.send_and_recv((src1, dst1), fn.copy_src('features', 'msg'), fn.sum('msg', 'h'), nfunc_layer1)\n",
    "        \n",
    "    g.send_and_recv((src2, dst2), fn.copy_src('h', 'msg'), fn.sum('msg', 'h'), nfunc_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neighbor sampling and NodeFlow\n",
    "\n",
    "DGL provides a set of sampling algorithms. These sampling algorithms create NodeFlows for mini-batch training on a graph. The code below creates a neighbor sampler that creates NodeFlows for 2-layer GCN. Each NodeFlow (mini-batch) has 3 target nodes and it samples at maximal 4 neighbors on each neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = iter(dgl.contrib.sampling.NeighborSampler(g, 3, num_neighs, neighbor_type='in', num_hops=2))\n",
    "nf = next(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Have a look at the data inside the NodeFlow.\n",
    "\n",
    "The NodeFlow should have 3 layers and 2 blocks.\n",
    "\n",
    "![title](https://s3.us-east-2.amazonaws.com/dgl.ai/amlc_tutorial/NodeFlow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf.num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf.num_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nodes in NodeFlow can be identified by two sets of Ids: global node Id (the node Ids in the original graph) and local node Id (used inside this NodeFlow). Local node Ids inside a NodeFlow are labelled starting from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nf.layer_nid(0).asnumpy())\n",
    "print(nf.layer_parent_nid(0).asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The NodeFlow should contain 3 target nodes to compute node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = nf.layer_parent_nid(-1).asnumpy()\n",
    "print(\"The target nodes in the batch: \", nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can double check that the nodes and edges in the NodeFlow are sampled from the parent graph. One way of checking it is to see the neighbors of the target nodes also exist in the neighborhoods of the target nodes in the parent graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_nodes = nf.layer_parent_nid(1).asnumpy()\n",
    "full_graph_neighbors = g.in_edges(nodes)[0].asnumpy()\n",
    "print(\"The nodes in the second layer: \", layer1_nodes)\n",
    "print(\"The in-neighbors of the nodes: \", np.sort(full_graph_neighbors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Get data from the parent graph\n",
    "\n",
    "When a NodeFlow is created from the neighbor sampler, it doesn't contain node data or edge data. We need to explicitly copy data from the parent graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(nf.layers[0].data['labels'])\n",
    "except KeyError:\n",
    "    print(\"labels don't exist in node data of the NodeFlow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf.copy_from_parent()\n",
    "print(nf.layers[-1].data['labels'].asnumpy())\n",
    "print(g.nodes[nf.layer_parent_nid(-1)].data['labels'].asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Trigger the computation on NodeFlow\n",
    "\n",
    "We can triger the computation on NodeFlow with `block_compute`, which performs the computation on a block. To compute the node embeddings of the target nodes, we start the block computation in block 0 and propogate to the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfunc_layers = []\n",
    "nfunc_layers.append(NodeUpdate(in_feats, n_hidden, mx.nd.relu))\n",
    "for i in range(nf.num_blocks - 1):\n",
    "    nfunc_layers.append(NodeUpdate(n_hidden, n_hidden, mx.nd.relu))\n",
    "for l in nfunc_layers:\n",
    "    l.initialize()\n",
    "\n",
    "nf.layers[0].data['h'] = nf.layers[0].data['features']\n",
    "for i in range(nf.num_blocks):\n",
    "    nf.block_compute(i, fn.copy_src('h', 'msg'), fn.sum('msg', 'h'), nfunc_layers[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Train GCN with neighbor sampling\n",
    "In an $L$-layer graph convolution network (GCN), given a graph $G=(V, E)$, represented as an adjacency matrix $A$, with node features $H^{(0)} = X \\in \\mathbb{R}^{|V| \\times d}$, the hidden feature of a node $v$ in $(l+1)$-th layer $h_v^{(l+1)}$ depends on the features of all its neighbors in the previous layer $h_u^{(l)}$:\n",
    "$$\n",
    "z_v^{(l+1)} = \\sum_{u \\in \\mathcal{N}(v)} \\tilde{A}_{uv} h_u^{(l)} \\qquad h_v^{(l+1)} = \\sigma ( z_v^{(l+1)} W^{(l)})\n",
    "$$\n",
    "where $\\mathcal{N}(v)$ is the neighborhood of $v$, $\\tilde{A}$ could be any normalized version of $A$ such as $D^{-1} A$ in Kipf et al., $\\sigma(\\cdot)$ is an activation function, and $W^{(l)}$ is a trainable parameter of the $l$-th layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Instead of using all the $L$-hop neighbors of a node $v$, [Hamilton et al.](https://arxiv.org/abs/1706.02216) propose *neighbor sampling*, which randomly samples a few neighbors $\\hat{\\mathcal{N}}^{(l)}(v)$ to estimate the aggregation $z_v^{(l+1)}$ of its total neighbors $\\mathcal{N}(v)$ in $l$-th GCN layer, by an unbiased estimator $\\hat{z}_v^{(l+1)}$\n",
    "$$\n",
    "\\hat{z}_v^{(l+1)} = \\frac{\\vert \\mathcal{N}(v) \\vert }{\\vert \\hat{\\mathcal{N}}^{(l)}(v) \\vert} \\sum_{u \\in \\hat{\\mathcal{N}}^{(l)}(v)} \\tilde{A}_{uv} \\hat{h}_u^{(l)} \\qquad\n",
    "\\hat{h}_v^{(l+1)} = \\sigma ( \\hat{z}_v^{(l+1)} W^{(l)} )\n",
    "$$\n",
    "Let $D^{(l)}$ be the number of neighbors to be sampled for each node at the $l$-th layer,\n",
    "then the receptive field size of each node can be controlled under $\\prod_{i=0}^{L-1} D^{(l)}$ by *neighbor sampling*.\n",
    "\n",
    "Here we define the node UDF which is a fully-connected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class GCNSampling(gluon.Block):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_blocks,\n",
    "                 activation,\n",
    "                 dropout,\n",
    "                 **kwargs):\n",
    "        super(GCNSampling, self).__init__(**kwargs)\n",
    "        self.dropout = dropout\n",
    "        with self.name_scope():\n",
    "            self.blocks = gluon.nn.Sequential()\n",
    "            # input block\n",
    "            self.blocks.add(NodeUpdate(in_feats, n_hidden, activation))\n",
    "            # hidden blocks\n",
    "            for i in range(1, n_blocks-1):\n",
    "                self.blocks.add(NodeUpdate(n_hidden, n_hidden, activation))\n",
    "            # output block\n",
    "            self.blocks.add(NodeUpdate(n_hidden, n_classes))\n",
    "\n",
    "    def forward(self, nf):\n",
    "        nf.layers[0].data['h'] = nf.layers[0].data['features']\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            h = nf.layers[i].data['h']\n",
    "            if self.dropout:\n",
    "                h = mx.nd.Dropout(h, p=self.dropout)\n",
    "            nf.layers[i].data['h'] = h\n",
    "            # block_compute() computes the feature of layer i given layer\n",
    "            # i-1, with the given message, reduce, and apply functions.\n",
    "            # Here, we essentially aggregate the neighbor node features in\n",
    "            # the previous layer, and update it with the `layer` function.\n",
    "            nf.block_compute(i,\n",
    "                             fn.copy_src(src='h', out='m'),\n",
    "                             lambda node : {'h': node.mailbox['m'].mean(axis=1)},\n",
    "                             block)\n",
    "        return nf.layers[-1].data.pop('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The code below puts everything together to train a 2-layer GCN in mini-batches with neighbor sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout probability\n",
    "dropout = 0.2\n",
    "# batch size\n",
    "batch_size = 100\n",
    "# number of neighbors to sample\n",
    "num_neighbors = 4\n",
    "# number of epochs\n",
    "num_epochs = 30\n",
    "\n",
    "# The number of classes we want to classify the nodes\n",
    "n_classes = data.num_labels\n",
    "# The number of layers of GCN.\n",
    "L = 2\n",
    "\n",
    "# initialize the model and cross entropy loss\n",
    "model = GCNSampling(in_feats, n_hidden, n_classes, L,\n",
    "                    mx.nd.relu, dropout, prefix='GCN')\n",
    "model.initialize()\n",
    "loss_fcn = gluon.loss.SoftmaxCELoss()\n",
    "\n",
    "# use adam optimizer\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "                        {'learning_rate': 0.03, 'wd': 0})\n",
    "\n",
    "labels = g.ndata['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for nf in dgl.contrib.sampling.NeighborSampler(g, batch_size,\n",
    "                                                   num_neighbors,\n",
    "                                                   neighbor_type='in',\n",
    "                                                   shuffle=True,\n",
    "                                                   num_hops=L,\n",
    "                                                   seed_nodes=train_nid):\n",
    "        # When `NodeFlow` is generated from `NeighborSampler`, it only contains\n",
    "        # the topology structure, on which there is no data attached.\n",
    "        # Users need to call `copy_from_parent` to copy specific data,\n",
    "        # such as input node features, from the original graph.\n",
    "        nf.copy_from_parent()\n",
    "        with mx.autograd.record():\n",
    "            # forward\n",
    "            pred = model(nf)\n",
    "            batch_nids = nf.layer_parent_nid(-1).astype('int64')\n",
    "            batch_labels = labels[batch_nids]\n",
    "            # cross entropy loss\n",
    "            loss = loss_fcn(pred, batch_labels)\n",
    "            loss = loss.sum() / len(batch_nids)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # optimization\n",
    "        trainer.step(batch_size=1)\n",
    "        print(\"Epoch[{}]: loss {}\".format(epoch, loss.asscalar()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
