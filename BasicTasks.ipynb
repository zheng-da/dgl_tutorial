{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import citegrh\n",
    "from dgl.nn.pytorch import conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout,\n",
    "                 aggregator_type):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.g = g\n",
    "\n",
    "        # input layer\n",
    "        self.layers.append(conv.SAGEConv(in_feats, n_hidden, aggregator_type, feat_drop=dropout, activation=activation))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(conv.SAGEConv(n_hidden, n_hidden, aggregator_type, feat_drop=dropout, activation=activation))\n",
    "        # output layer\n",
    "        self.layers.append(conv.SAGEConv(n_hidden, n_classes, aggregator_type, feat_drop=dropout, activation=None)) # activation None\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            h = layer(self.g, h)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess dataset\n",
    "data = citegrh.load_pubmed()\n",
    "features = torch.FloatTensor(data.features)\n",
    "labels = torch.LongTensor(data.labels)\n",
    "train_mask = torch.ByteTensor(data.train_mask)\n",
    "val_mask = torch.ByteTensor(data.val_mask)\n",
    "test_mask = torch.ByteTensor(data.test_mask)\n",
    "in_feats = features.shape[1]\n",
    "n_classes = data.num_labels\n",
    "n_edges = data.graph.number_of_edges()\n",
    "print(\"\"\"----Data statistics------'\n",
    "      #Edges %d\n",
    "      #Classes %d\n",
    "      #Train samples %d\n",
    "      #Val samples %d\n",
    "      #Test samples %d\"\"\" %\n",
    "          (n_edges, n_classes,\n",
    "           train_mask.sum().item(),\n",
    "           val_mask.sum().item(),\n",
    "           test_mask.sum().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = data.graph\n",
    "print(g.number_of_nodes(), g.number_of_edges())\n",
    "g.remove_edges_from(g.selfloop_edges())\n",
    "print(g.number_of_nodes(), g.number_of_edges())\n",
    "g = DGLGraph(g)\n",
    "g.readonly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 16\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "aggregator_type = 'gcn'\n",
    "weight_decay = 5e-4\n",
    "n_epochs = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create GraphSAGE model\n",
    "gconv_model = GraphSAGE(g,\n",
    "                        in_feats,\n",
    "                        n_hidden,\n",
    "                        n_classes,\n",
    "                        n_layers,\n",
    "                        F.relu,\n",
    "                        dropout,\n",
    "                        aggregator_type)\n",
    "\n",
    "class NodeClassification(nn.Module):\n",
    "    def __init__(self, gconv_model):\n",
    "        super(NodeClassification, self).__init__()\n",
    "        self.gconv_model = gconv_model\n",
    "        self.loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, features, train_mask):\n",
    "        logits = self.gconv_model(features)\n",
    "        return self.loss_fcn(logits[train_mask], labels[train_mask])\n",
    "\n",
    "# Node classification task\n",
    "model = NodeClassification(gconv_model)\n",
    "    \n",
    "def evaluate(model, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = gconv_model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first split the graph into the training set and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eids = np.random.permutation(g.number_of_edges())\n",
    "train_eids = eids[:int(len(eids) * 0.8)]\n",
    "test_eids = eids[int(len(eids) * 0.8):]\n",
    "train_g = g.edge_subgraph(train_eids, preserve_nodes=True)\n",
    "test_g = g.edge_subgraph(test_eids, preserve_nodes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct negative edges for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_sample(g, neg_sample_size, edges=None):\n",
    "    sampler = dgl.contrib.sampling.EdgeSampler(g, batch_size=g.number_of_edges(),\n",
    "                                               seed_edges=edges,\n",
    "                                               neg_sample_size=neg_sample_size,\n",
    "                                               negative_mode='tail',\n",
    "                                               return_false_neg=True)\n",
    "    sampler = iter(sampler)\n",
    "    return next(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "neg_sample_size = 100\n",
    "lr = 1e-1\n",
    "n_layers = 1\n",
    "\n",
    "# create GraphSAGE model\n",
    "gconv_model = GraphSAGE(train_g,\n",
    "                        in_feats,\n",
    "                        n_hidden,\n",
    "                        16,\n",
    "                        n_layers,\n",
    "                        F.relu,\n",
    "                        dropout,\n",
    "                        aggregator_type)\n",
    "\n",
    "class LinkPrediction(nn.Module):\n",
    "    def __init__(self, gconv_model):\n",
    "        super(LinkPrediction, self).__init__()\n",
    "        self.gconv_model = gconv_model\n",
    "\n",
    "    def forward(self, features, train_mask):\n",
    "        emb = self.gconv_model(features)\n",
    "        pos_g, neg_g = neg_sample(g, neg_sample_size)\n",
    "        pos_src, pos_dst = pos_g.all_edges()\n",
    "        pos_heads = emb[pos_src]\n",
    "        pos_tails = emb[pos_dst]\n",
    "        neg_src, neg_dst = neg_g.all_edges()\n",
    "        neg_heads = emb[neg_src].reshape(-1, neg_sample_size, emb.shape[1])\n",
    "        neg_tails = emb[neg_dst].reshape(-1, neg_sample_size, emb.shape[1])\n",
    "        assert neg_heads.shape[0] == neg_tails.shape[0]\n",
    "        pos_score = F.logsigmoid(torch.sum(pos_heads * pos_tails, dim=1))\n",
    "        neg_score = F.logsigmoid(-torch.sum(neg_heads * neg_tails, dim=2))\n",
    "        return torch.mean(-pos_score - torch.sum(neg_score, dim=1))\n",
    "    \n",
    "\n",
    "# Link prediction task\n",
    "model = LinkPrediction(gconv_model)\n",
    "\n",
    "def evaluate(model, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        emb = gconv_model(features)\n",
    "        \n",
    "        pos_g, neg_g = neg_sample(g, neg_sample_size, test_eids)\n",
    "        pos_src, pos_dst = pos_g.all_edges()\n",
    "        pos_heads = emb[pos_src]\n",
    "        pos_tails = emb[pos_dst]\n",
    "        neg_src, neg_dst = neg_g.all_edges()\n",
    "        neg_heads = emb[neg_src].reshape(-1, neg_sample_size, emb.shape[1])\n",
    "        neg_tails = emb[neg_dst].reshape(-1, neg_sample_size, emb.shape[1])\n",
    "        filter_bias = neg_g.edata['false_neg'].reshape(-1, neg_sample_size)\n",
    "\n",
    "        pos_score = F.logsigmoid(torch.sum(pos_heads * pos_tails, dim=1))\n",
    "        neg_score = F.logsigmoid(torch.sum(neg_heads * neg_tails, dim=2))\n",
    "        neg_score -= filter_bias.float()\n",
    "        pos_score = pos_score.unsqueeze(1)\n",
    "        rankings = torch.sum(neg_score > pos_score, dim=1) + 1\n",
    "        print('MR:', np.mean(rankings.numpy()))\n",
    "        print('MRR:', np.mean(1.0/rankings.numpy()))\n",
    "        return np.mean(1.0/rankings.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# initialize graph\n",
    "dur = []\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    if epoch >= 3:\n",
    "        t0 = time.time()\n",
    "    # forward\n",
    "    loss = model(features, train_mask)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >= 3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    acc = evaluate(model, features, labels, val_mask)\n",
    "    print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f} | \"\n",
    "            \"ETputs(KTEPS) {:.2f}\".format(epoch, np.mean(dur), loss.item(),\n",
    "                                            acc, n_edges / np.mean(dur) / 1000))\n",
    "\n",
    "print()\n",
    "acc = evaluate(model, features, labels, test_mask)\n",
    "print(\"Test Accuracy {:.4f}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
