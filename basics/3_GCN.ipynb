{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Graph Convolutional Network (GCN) in DGL\n",
    "=====================================================\n",
    "\n",
    "Graph convolutional network (GCN) is a popular model proposed by [Kipf & Welling](https://arxiv.org/abs/1609.02907) to encode graph structure by message passing. The high-level idea is similar to our toy task: node features are updated by aggregating the messages from the neighbors. Here is its message passing equation:\n",
    "\n",
    "$$\n",
    "h_{v_i}^{(l+1)} = \\sigma \\left(\\sum_{j\\in\\mathcal{N}(i)}\\frac{1}{c_{ij}}h_{v_j}^{(l)}W^{(l)} \\right)\n",
    "$$\n",
    "\n",
    ", where $v_i$ is any node in the graph; $h_{v_i}$ is the feature of node $v_i$; $\\mathcal{N}(i)$ denotes the neighborhood of $v_i$; $c_{ij}$ is the normalization constant related to node degrees; $W$ is the parameter and $\\sigma$ is a non-linear activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of setup, just ignore this cell\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8.0, 6.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['animation.html'] = 'html5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps to implement GCN in DGL is also similar to the toy task (2_MessagePassing.ipynb):\n",
    "* Define the message function.\n",
    "* Define the reduce function.\n",
    "* Define how they are triggered using `send` and `recv`.\n",
    "\n",
    "We first pretend that we already have implemented the message function `gcn_message` and reduce function `gcn_reduce`, and look at how we can define GCN layer and trigger computation with `send` and `recv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "\n",
    "# Define the GCN module\n",
    "class GCN(gluon.Block):\n",
    "    def __init__(self, out_feats):\n",
    "        super(GCN, self).__init__()\n",
    "        self.linear = nn.Dense(out_feats)\n",
    "    \n",
    "    def forward(self, g, inputs):\n",
    "        # g is the graph and the inputs is the input node features\n",
    "        # first perform linear transformation\n",
    "        h = self.linear(inputs)\n",
    "        # set the node features\n",
    "        g.ndata['h'] = h\n",
    "        # trigger message passing, gcn_message and gcn_reduce will be defined later\n",
    "        g.update_all(gcn_message, gcn_reduce)\n",
    "        # get the result node features\n",
    "        h = g.ndata.pop('h')\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fill in the missing message and reduce function. For simplicity, for now, we ignore the normalization constant $c_{ij}$.\n",
    "\n",
    "From the equation of GCN above:\n",
    "- each node sends out the embedding after linear transformation to their neighbors, so the message from node $u$ to node $v$ can be computed as\n",
    "$$m_{uv} = h_u$$\n",
    "- each node aggregates received messages by summation, so the aggregated messages on node $v$ can be computed as\n",
    "$$a_v = \\sum\\limits_{u\\in \\mathcal{N}(v)}m_{uv}$$, where $\\mathcal{N}(v)$ is the neighbor set of node $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "Follow the two equations above and finish the message & reduce function for Graph Convolutional Network based on the equation above.\n",
    "\n",
    "NOTE: for now, we ignore the normalization factor $c_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> YOUR CODE STARTS\n",
    "\n",
    "# >>> YOUR CODE ENDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will still use karate club as example. Let's use the helpful utility function to load the graph and make it bidirectional with self loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import networkx as nx\n",
    "from tutorial_utils import create_karate_graph, convert_to_bidirectional\n",
    "G = create_karate_graph()\n",
    "GG = convert_to_bidirectional(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this model, let's try to predict which club member will join whose group (instructor or club president) after the split. We adopt the semi-supervised setting developed by Kipf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet.ndarray as nd\n",
    "import numpy as np\n",
    "\n",
    "# Define a 2-layer GCN model\n",
    "class Model(gluon.Block):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.gcn1 = GCN(hidden_size)\n",
    "        self.gcn2 = GCN(num_classes)\n",
    "    \n",
    "    def forward(self, g, inputs):\n",
    "        h = self.gcn1(g, inputs)\n",
    "        h = nd.relu(h)\n",
    "        h = self.gcn2(g, h)\n",
    "        return h\n",
    "\n",
    "inputs = nd.eye(34)  # featureless inputs\n",
    "labeled_nodes = nd.array([0, 33], dtype=np.int64)  # only the instructor and the president nodes are labeled\n",
    "labels = nd.array([0, 1], dtype=np.int64)  # their labels are different\n",
    "model = Model(5, 2)\n",
    "model.initialize()\n",
    "loss_fn = gluon.loss.SoftmaxCELoss()\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': 0.01})\n",
    "\n",
    "all_logits = []\n",
    "for epoch in range(30):\n",
    "    with mxnet.autograd.record():\n",
    "        logits = model(GG, inputs)\n",
    "        loss = loss_fn(logits[labeled_nodes], labels).sum()\n",
    "    \n",
    "    loss.backward()\n",
    "    trainer.step(batch_size=1)\n",
    "    \n",
    "    all_logits.append(logits.detach())\n",
    "    print('Epoch %d | Loss: %.4f' % (epoch, loss.asscalar()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the node classification using the logits output.\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig = plt.figure(dpi=150)\n",
    "fig.clf()\n",
    "ax = fig.subplots()\n",
    "nx_G = G.to_networkx()\n",
    "def draw(i):\n",
    "    cls1color = '#00FFFF'\n",
    "    cls2color = '#FF00FF'\n",
    "    pos = {}\n",
    "    colors = []\n",
    "    for v in range(34):\n",
    "        pos[v] = all_logits[i][v].asnumpy()\n",
    "        cls = np.argmax(pos[v])\n",
    "        colors.append(cls1color if cls else cls2color)\n",
    "    ax.cla()\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Epoch: %d' % i)\n",
    "    nx.draw(nx_G.to_undirected(), pos, node_color=colors, with_labels=True, node_size=500)\n",
    "\n",
    "ani = animation.FuncAnimation(fig, draw, frames=len(all_logits), interval=200)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "There is still one missing piece. In our GCN model, \n",
    "$$\n",
    "h_{v_i}^{(l+1)} = \\sigma \\left(\\sum_{j\\in\\mathcal{N}(i)}\\frac{1}{c_{ij}}h_{v_j}^{(l)}W^{(l)} \\right)\n",
    "$$\n",
    "And we haven't implemented the normalizer $c_{ij}$. Kipf, in GCN paper, pointed out that the normalizer should be computed as follows:\n",
    "\n",
    "$$\n",
    "c_{ij} = \\sqrt{d_id_j}\n",
    "$$\n",
    "\n",
    ", where $d_i, d_j$ are the degrees of node $v_i$ and $v_j$ respectively. Your task is to modify the program to implement it.\n",
    "\n",
    "**Hint #1**: Use `GG.in_degrees(GG.nodes())` to get a 1-D tensor containing the degrees of all the nodes.\n",
    "\n",
    "**Hint #2**: Since $c_{ij}$ has a subscription $ij$, it is tied to the edges, and our message function is (not coincidently) an **edge UDF**.\n",
    "\n",
    "Have fun :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> YOUR CODE STARTS\n",
    "\n",
    "# <<< YOUR CODE ENDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
